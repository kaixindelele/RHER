# RHER: (Ralay-HER)--A Powerful Variant of HER
The official code for the paper â€œ[Relay Hindsight Experience Replay: Self-Guided Continual Reinforcement Learning for Sequential Object Manipulation Tasks with Sparse Rewards](https://arxiv.org/abs/2208.00843)â€

è®ºæ–‡æœ€æ–°ç‰ˆåœ¨ä»“åº“çš„[RHER_git.pdf](https://github.com/kaixindelele/RHER/blob/main/RHER_Git.pdf)ä¸­å¯ä»¥æŸ¥çœ‹ã€‚

å‰ä¸€ç‰ˆæœ¬çš„ä¸­æ–‡ç‰ˆå¯ä»¥åœ¨[RHER_old_ä¸­æ–‡ç‰ˆ](https://github.com/kaixindelele/RHER/blob/main/RHER-old-%E4%B8%AD%E6%96%87%E7%89%88.pdf)æŸ¥çœ‹ã€‚

æ¬¢è¿**å¼•ç”¨**å’Œè®¨è®ºç»†èŠ‚ã€‚

ğŸ’¥ğŸ’¥ğŸ’¥<strong> 7.24. æœ¬æ–‡å·²è¢«Neurocomputingæ¥æ”¶ï¼Œæ„Ÿè°¢ä¸€è·¯ä»¥æ¥æ‰€æœ‰å¸®åŠ©å’Œæ”¯æŒçš„æœ‹å‹å’Œè€å¸ˆï¼
</strong>


ğŸ’¥ğŸ’¥ğŸ’¥<strong> 7.10. 
It is noteworthy that in a recent work, [RoMo-HER](https://arxiv.org/abs/2306.16061), based on the RHER, has combined model-based schemes to further improve sample efficiency in the classic tasks of FetchPush and FetchPickAndPlace.

Moreover, it can be seen from the paper that the authors have **independently reproduced the performance of our open-source code**.

Lastly, based on the experimental results, even with the addition of model-based approaches, the improvement in sample efficiency is still limited, demonstrating that RHER is indeed highly efficient in these two standard tasks.

 </strong>


 # Starchart

[![Star History Chart](https://api.star-history.com/svg?repos=kaixindelele/RHER&type=Date)](https://star-history.com/#kaixindelele/RHER&Date)


 ![~FAYHELW24F4F{G57ZBT}GX](https://github.com/kaixindelele/RHER/assets/28528386/86fb2a22-412f-4ebd-9364-780424997646)


 æ²¡æƒ³åˆ°ï¼ŒçœŸçš„æœ‰åŒå­¦ï¼ŒåŸºäºRHERåšç®—æ³•æ”¹è¿›ï¼
 
 å½“åˆè¯´åœ¨readmeè¯´ï¼Œå¸Œæœ›RHERèƒ½æˆä¸ºæ–°çš„benchmarkï¼Œå¥½åƒéšçº¦èƒ½çœ‹åˆ°ç‚¹å¸Œæœ›äº†ã€‚

 ä½œä¸ºä¸€ä¸ªå»¶æ¯•çš„åšå£«ç”Ÿï¼Œçœ‹åˆ°è¿™ç¯‡å·¥ä½œèƒ½å¤Ÿå¾—åˆ°åˆ«äººçš„è®¤å¯ï¼Œä»¥åŠå¯¹é¢†åŸŸçš„å¾®å°å¸®åŠ©ï¼Œè¿˜æ˜¯æœ‰ç‚¹è«åçš„æ„Ÿæ…¨ã€‚
 
 æœ€åå¸Œæœ›RHERä¸­ä¸€äº›æœ‰æ„æ€çš„æ“ä½œï¼Œå¯ä»¥å¯¹å…¶ä»–é¢†åŸŸæœ‰ä¸€äº›å¯å‘ï¼Œæ¯”å¦‚å¤šæ™ºèƒ½ä½“çš„åˆä½œï¼Œæ¯”å¦‚é€†å¼ºåŒ–å­¦ä¹ ï¼Œæ¯”å¦‚â€œæˆ‘çš„ä¸–ç•Œâ€è¿™ç§å¤æ‚åºåˆ—ä»»åŠ¡çš„æ¢ç´¢ï¼Œæ¯”å¦‚åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å¼•å¯¼æ¢ç´¢ã€‚

 æ„¿ä¸–ç•Œç¾å¥½
 

## æœ€æ–°ä¸­æ–‡ç‰ˆç¢ç¢å¿µï¼RHERå¯¹æˆ‘çš„åå‘æŒ‡å¯¼ï¼š2023-06-23-00am
<details><summary><code>æŸ¥çœ‹æœ€æ–°ä¸­æ–‡ç‰ˆç¢ç¢å¿µ</code></summary>
  ç»¼åˆæ¥çœ‹RHERå¯¹æˆ‘çš„åå‘æŒ‡å¯¼æœ‰ä»¥ä¸‹å‡ ç‚¹ï¼š
  ï¼‘.ã€€ç›®æ ‡å¯ä»¥æ˜¯è¿œå¤§çš„ï¼Œä½†è¦ä»å¤Ÿä¸€å¤Ÿèƒ½å¤Ÿçš„ç€çš„å­ç›®æ ‡å¼€å§‹åŠªåŠ›ï¼Œè¿™æ ·ä¼šäº‹åŠåŠŸå€ï¼Œæ•ˆç‡æ¯”è¾ƒé«˜
  ï¼’.ã€€å¯¹äºæœ‰äººæ“…é•¿çš„é¢†åŸŸï¼Œè‡ªå·±ä¸ç†Ÿæ‚‰çš„é¢†åŸŸï¼Œè¦å­¦ä¼šå¯»æ±‚ä¸“å®¶æ™ºèƒ½ä½“çš„æ‰‹æŠŠæ‰‹æ•™å­¦ï¼Œè¿™æ ·æ•ˆç‡æé«˜
  ï¼“.ã€€æŠ€èƒ½ã€ç»éªŒã€èµ„æºå’Œè®¤çŸ¥çš„ç§¯ç´¯ï¼Œå¯¹ä½ å®Œæˆå¤æ‚ä»»åŠ¡æ¥è¯´ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªå¿…è¦çš„è¿‡ç¨‹
  ï¼”.ã€€åœ¨è‡ªå·±æ¢ç´¢å¤æ‚ä»»åŠ¡å‰ï¼Œè¦å……åˆ†æ„è¯†åˆ°ç›®æ ‡å’Œè‡ªèº«èƒ½åŠ›ä¸­é—´çš„å·®è·ï¼Œå­¦ä¼šåˆ‡åˆ†å­ä»»åŠ¡ï¼Œå­¦ä¼šå¯»æ±‚ä¸“å®¶æ™ºèƒ½ä½“çš„å¸®åŠ©å’ŒæŒ‡å¯¼ã€‚
  
  æœ€è¿‘ç†Ÿæ‚‰çš„æœ‹å‹ä»¬éƒ½é¡ºåˆ©æ¯•ä¸šäº†ï¼Œè€Œæˆ‘è¿˜åœ¨ç­‰è¿™ç¯‡å·¥ä½œè¢«æ­£å¼æ¥æ”¶ï¼Œæ‰èƒ½æ¯•ä¸šï¼Œå¤œæ·±æ—¶åˆ»ï¼Œè®°å½•ä¸€ä¸‹æœ€è¿‘çš„æ„Ÿæ‚Ÿã€‚
  
  è¯»åšæ˜¯æˆ‘è‡ªå·±çš„ä¸€ä¸ªé‡å¤§æŠ‰æ‹©ï¼Œ2018å¹´åº•çš„æ—¶å€™ï¼Œæˆ‘è®¤ä¸ºå†³ç­–ï¼Œæ˜¯çœŸæ­£çš„æ™ºèƒ½ï¼Œè€Œå½“æ—¶å¼ºåŒ–åˆšå¼€å§‹å…´èµ·ã€‚
  
  æˆ‘æ„¿æ„èŠ±å››å¹´æ—¶é—´å»æ¢ç´¢è¿™ä¸ªé¢†åŸŸï¼Œå¸Œæœ›èƒ½åœ¨åšå£«æœŸé—´ï¼Œåšå‡ºç‚¹æ‹“å®½äººç±»è®¤çŸ¥è¾¹ç•Œçš„å·¥ä½œã€‚
  
  ç°åœ¨å·²ç»å››å¹´æ•´äº†ï¼Œæˆ‘å…¶å®è¿˜æ˜¯åœ¨å’Œè‡ªå·±çš„è®¤çŸ¥ä½œæ–—äº‰ã€‚
  
  æˆ‘çš„ç ”ç©¶å†…å®¹å…¶å®æ˜¯æˆ‘è‡ªå·±ç ”ç©¶è¿‡ç¨‹çš„ä¸€ä¸ªéªŒè¯ã€‚
  
  æ¯”å¦‚è¯´ï¼Œæˆ‘åœ¨å—åˆ°å¯¼å¸ˆæŒ‡å¯¼çš„è¿‡ç¨‹ä¸­å‘ç°ï¼Œåˆšå¼€å§‹çš„æ—¶å€™ï¼Œå¯¼å¸ˆçš„ç»éªŒæ˜¯éå¸¸é è°±çš„ï¼Œä½†éšç€æˆ‘å­¦ä¹ çš„è¿‡ç¨‹ï¼Œå¯¼å¸ˆçš„é¢†åŸŸçŸ¥è¯†å·²ç»é€æ¸OODäº†ï¼Œé‚£ä¹ˆæ¥ä¸‹æ¥æ¯æ¬¡ç»„ä¼šçš„è¯¦ç»†æŒ‡å¯¼ï¼Œéƒ½å¯èƒ½ä¸ä¼šæœ‰æ›´å¥½çš„æ¢ç´¢ç»“æœã€‚
  
  é‚£ä¹ˆæˆ‘å°±æå‡ºäº†æˆ‘çš„ç¬¬ä¸€ç¯‡æœ‰æ„æ€çš„å·¥ä½œï¼Œdense2sparseï¼Œå…ˆdenseå¥–åŠ±å‡½æ•°ï¼Œæ‰‹æŠŠæ‰‹çš„æ•™å­¦ï¼Œç„¶åå†sparseå¥–åŠ±å‡½æ•°è¿›è¡Œç­–ç•¥æé«˜ã€‚
  
  å“ˆå“ˆï¼Œè¿™ä¸ªMotivationæ˜¯æˆ‘çš„è®ºæ–‡é‡Œé¢æ— æ³•è¯´çš„ï¼Œå¸Œæœ›å¯¼å¸ˆçœ‹åˆ°äº†ï¼Œåº”è¯¥ä¹Ÿèƒ½ç†è§£ã€‚
  
  åè¿‡æ¥ï¼Œæˆ‘çš„ç ”ç©¶ï¼Œå¯¹æˆ‘è‡ªå·±çš„å­¦ä¹ å’Œç”Ÿæ´»ï¼Œä¹Ÿèƒ½èµ·åˆ°ä¸€ä¸ªåå‘çš„æŒ‡å¯¼ä½œç”¨ã€‚  
  
  æ¯”å¦‚è¯´RHERé‡Œé¢éšå«çš„ä¸€äº›é“ç†ï¼Œåœ¨æˆ‘åšChatPaperçš„æ—¶å€™ï¼Œå°±æœ‰å¾ˆå¤šä½“ç°ã€‚
  
  ä»2017å¹´çš„æ—¶å€™ï¼Œæˆ‘å°±å¸Œæœ›èƒ½å¤Ÿå»ºç«‹ä¸€ä¸ªå­¦æœ¯è®ºæ–‡ç¿»è¯‘å¹³å°ï¼Œå¸Œæœ›å„ä½ä½œè€…èƒ½å¤Ÿå°†è‡ªå·±å†™çš„è®ºæ–‡ï¼Œç¿»è¯‘æˆtaçš„æ¯è¯­ç‰ˆæœ¬ï¼Œæ–¹ä¾¿æœ¬å›½å¹¿å¤§çš„æ™®é€šå·¥ç¨‹å¸ˆå’Œä½å¹´çº§ï¼Œæˆ–è€…è·¨å­¦ç§‘çš„å…¶ä»–ç ”ç©¶è€…é˜…è¯»ã€‚
  
  ä½†æ˜¯åœ¨å½“æ—¶ï¼Œå¯¹æˆ‘è¿™ä¸ªæ™ºèƒ½ä½“æ¥è¯´ï¼Œè¿™æ˜¯ä¸€ä¸ªæ— æ³•å®ç°çš„ç›®æ ‡ã€‚
  
  æˆ‘æ— è®ºåšä»€ä¹ˆï¼Œéƒ½æ²¡æ³•å®ç°å®ƒï¼Œç”šè‡³äºï¼Œæˆ‘çš„è¡Œä¸ºå¯¹ç»“æœéƒ½ä¸ä¼šæœ‰æ˜¾è‘—çš„å½±å“ã€‚
  
  ä¸€èˆ¬æ¥è¯´ï¼Œæˆ‘ä¸æ˜¯ä¸€ä¸ªåŒæ ‡çš„äººï¼Œå¸Œæœ›åˆ«äººä»˜å‡ºçš„åŒæ—¶ï¼Œæˆ‘ä¸€èˆ¬ä¹Ÿä¼šè¦æ±‚æˆ‘è‡ªå·±å…ˆåšåˆ°ã€‚

  æˆ‘å¸Œæœ›å¤§å®¶ä¸€èµ·ç¿»è¯‘è‹±æ–‡è®ºæ–‡ï¼Œé‚£å°±ä»æˆ‘å¼€å§‹åšèµ·ã€‚
  
  è€Œæˆ‘åªèƒ½åšæˆ‘èƒ½åšçš„äº‹æƒ…ï¼Œæˆ‘åšæŒåœ¨çŸ¥ä¹å’ŒCSDNä¸Šåˆ†äº«æˆ‘è‡ªå·±çš„é˜…è¯»ç¬”è®°ï¼Œåˆ†äº«æˆ‘è‡ªå·±çš„å­¦ä¹ æ•™ç¨‹ã€‚  
  
  æˆ‘ä¹ŸåšæŒå¼€æºï¼Œä¸€ç›´ç»´æŠ¤DRLibå¼€æºä»“åº“ï¼Œä¸ºç¤¾åŒºæä¾›ä¸€å¥—æ¯”è¾ƒå¥½ç”¨çš„å¼ºåŒ–ç”»å›¾çš„è„šæœ¬ï¼Œä¸€å¥—æ¯”è¾ƒå¥½ç”¨çš„HERçš„å¤ç°ä»£ç ã€‚
  
  åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œæˆ‘è‡ªå·±çš„èƒ½åŠ›ä¹Ÿæœ‰äº†å°‘è®¸æå‡ï¼Œæˆ‘åœ¨ç¤¾åŒºä¹Ÿæœ‰äº†å°‘è®¸çš„å½±å“åŠ›ã€‚
  
  ç­‰åˆ°2023å¹´3æœˆ1å·ï¼Œæˆ‘æŠŠæ‰‹å¤´ä¸Šçš„ä¸¤ç¯‡å°è®ºæ–‡æŠ•å‡ºå»ä¹‹åï¼Œæˆ‘å¼€å§‹å­¦ä¹ LLMçš„çŸ¥è¯†ï¼Œä¸€æ–¹é¢æ„Ÿæ…¨æ–°çŸ¥è¯†çš„çˆ†ç‚¸ï¼Œå¦å¤–ä¸€æ–¹é¢ä¹Ÿå‘ç°ChatGPTçš„APIä¸ºäº†æˆ‘çš„ç›®æ ‡æä¾›äº†ä¸€ä¸ªæ–°çš„æœºä¼šï¼Œæˆ‘å¯ä»¥åˆ©ç”¨Chatæ¥åšè‹±æ–‡è®ºæ–‡çš„æ€»ç»“ï¼
  
  ç°åœ¨çš„å½¢åŠ¿å˜äº†ï¼Œä¸ä»…ç¯å¢ƒå˜äº†ï¼Œæˆ‘ä¹Ÿå˜äº†ã€‚
  
  ç¯å¢ƒæä¾›äº†ä¸€ä¸ªæ–°æœºä¼šï¼Œæˆ‘è‡ªå·±çš„çŸ¥è¯†ã€æŠ€èƒ½ã€è®¤çŸ¥ã€èµ„æºä¹Ÿæœ‰äº†æé«˜ã€‚
  
  æˆ‘è¿˜æŠŠç›®æ ‡ä»å…¨æ–‡ç¿»è¯‘ï¼Œé™ä½åˆ°äº†å…¨æ–‡æ€»ç»“ã€‚æˆ‘çš„æŠ€æœ¯æ ˆä»ç¼–ç¨‹å°ç™½ï¼Œå˜æˆäº†ç¨å¾®æ‡‚äº›çˆ¬è™«ï¼Œæ•°æ®å¤„ç†ï¼Œå¹¶å‘çš„å°ç™½ã€‚
  
  å› æ­¤æˆ‘ä»3.5å·å¼€å§‹å¼€å‘ChatPaperçš„åŸå‹ä»£ç ï¼Œåœ¨GPTå’Œæœ‹å‹çš„å¸®åŠ©ä¸‹ï¼Œ3.8å·åŸºæœ¬ä¸Šå°±åšå‡ºæ¥äº†ã€‚
  
  ç»è¿‡ä¸‰å››å¹´çš„ç§‘ç ”è®­ç»ƒï¼Œæˆ‘çŸ¥é“è®ºæ–‡çš„æ€»ç»“ï¼Œéœ€è¦å“ªäº›å…³é”®çš„å†…å®¹ï¼Œå› æ­¤æˆ‘å¯ä»¥å†™å‡ºä¸€äº›æ¯”è¾ƒé è°±çš„promptsã€‚
  
  ç¬¬ä¸€æ¬¡çœ‹åˆ°GPTæ€»ç»“å‡ºæ¥çš„è®ºæ–‡æ•ˆæœï¼Œè®©æˆ‘æ„Ÿåˆ°ååˆ†çš„æƒŠè‰³ã€‚
  
  æˆ‘ç»´æŠ¤çš„å¼ºåŒ–QQç¾¤ç¾¤å‹çš„åé¦ˆï¼Œè®©æˆ‘æ„Ÿè§‰åˆ°ï¼Œä¹Ÿè®¸è¿™æ˜¯ä¸€ä¸ªå¤§å®¶éƒ½éœ€è¦çš„å·¥å…·ã€‚
  
  å‘åˆ°çŸ¥ä¹ä¸Šï¼ŒçŸ¥ä¹çš„å…³æ³¨è€…çš„åé¦ˆä¹Ÿéå¸¸å¥½ã€‚
  
  å½“æ—¶ç±»ä¼¼çš„å·¥å…·åªæœ‰chatpdfï¼Œä¸”ä»–ä»¬å¹¶æ²¡æœ‰é’ˆå¯¹å­¦æœ¯åšä¼˜åŒ–ï¼Œä¹Ÿæ— æ³•å…¨æ–‡æ€»ç»“ã€‚
  
  å› æ­¤ï¼Œæˆ‘æ„Ÿè§‰æˆ‘åšå‡ºäº†ä¸€ä¸ªæ¯”è¾ƒæœ‰ç”¨çš„å·¥å…·ã€‚
  
  ç»è¿‡4å¤©çš„ä½¿ç”¨å’Œä¼˜åŒ–ï¼Œæˆ‘äº3.12å·å¼€æºåˆ°GitHubä¸Šã€‚
  
  ä»è¿™ä¸€åˆ»å¼€å§‹ï¼Œæˆ‘ä½“ä¼šåˆ°äº†ç¯å¢ƒå¿«é€Ÿåé¦ˆï¼Œä¸”denseå¥–åŠ±çš„å¿«ä¹äº†ã€‚
  
  å› ä¸ºæˆ‘ä¹‹å‰çš„ç¤¾åŒºï¼Œä»¥åŠçŸ¥ä¹å’ŒBç«™ä¸‰å¹´ç§¯ç´¯ä¸‹æ¥çš„å…³æ³¨è€…ï¼Œå¸®æˆ‘åšäº†ç¬¬ä¸€è½®çš„æµé‡æ¨å¹¿ã€‚
  
  3.13å·ï¼ŒHuggingFaceçš„AKå¤§ä½¬è½¬å‘äº†ChatPaperã€‚
  
  å¤šæ–¹å¸®åŠ©ä¸‹ï¼Œå‰ä¸¤å¤©é¡¹ç›®çš„staræ•°è¿…é€Ÿä¸Šå‡åˆ°äº†ä¸€ä¸¤ç™¾ã€‚
  
  3.15å·ï¼Œä¸çŸ¥é“æ˜¯å“ªä½å¤§ä½¬çš„å…³ç…§ï¼Œä¸Šäº†GitHubçš„çƒ­æ¦œï¼Œç›´åˆ°3.18å·ã€‚
  
  è¿™æ—¶å€™staræ•°æŒ‡æ•°çº§ä¸Šå‡ï¼Œç›´å¥”1K starã€‚
  
  è¯»åšå››å¹´ï¼Œæˆ‘ç¬¬ä¸€æ¬¡å……åˆ†çš„æ„Ÿå—åˆ°å·¥ä½œè¢«è®¤å¯çš„æˆå°±æ„Ÿï¼Œè™½ç„¶ä¸æ˜¯æˆ‘çš„å­¦æœ¯æˆæœã€‚

  ä¹‹åçš„å‘å±•å°±éå¸¸æœ‰æ„æ€äº†ï¼Œä½œä¸ºç¬¬ä¸€ä¸ªå°†Chatåº”ç”¨åˆ°ç§‘ç ”é¢†åŸŸçš„å¼€æºé¡¹ç›®ï¼Œæœ‰éå¸¸å¤šçš„åŒå­¦å¸®åŠ©æˆ‘ä¸€èµ·ç»´æŠ¤è¿™ä¸ªé¡¹ç›®ã€‚
  
  ç„¶åæˆ‘å¼€å§‹å°è¯•å¸¦å›¢é˜Ÿï¼Œä½†å¾ˆæ˜æ˜¾ï¼Œæˆ‘çš„æŠ€èƒ½ã€è®¤çŸ¥å’Œç»éªŒï¼Œå·²ç»ä¸è¶³ä»¥å¤„ç†è¿™ç§çªç„¶å‡ºç°çš„æ–°ä»»åŠ¡äº†ã€‚
  
  ä½†æ™ºèƒ½ä½“ æ™ºèƒ½ çš„ä½“ç°åœ¨äºå’Œç¯å¢ƒçš„äº¤äº’å’Œå­¦ä¹ ã€‚
  
  è€Œæˆ‘çš„ç­–ç•¥åˆ™æ˜¯ï¼Œåä¿å®ˆçš„æ¢ç´¢ï¼Œè™½ç„¶ç°åœ¨å›è¿‡å¤´æ¥çœ‹ï¼Œæˆ‘å¯èƒ½é”™è¿‡äº†ä¸€äº›æœºä¼šï¼Œæˆ–è€…æœ‰ä¸€äº›å†³ç­–æ˜¯éæœ€ä¼˜çš„ã€‚
  
  ä½†æ²¡æœ‰åŠæ³•ï¼Œäººç”Ÿæ— æ³•resetï¼Œæˆ‘åªæœ‰ä¸€æ¬¡æ¢ç´¢æœºä¼šï¼Œæˆ‘çš„è®¤çŸ¥ä¸å¤Ÿçš„æƒ…å†µä¸‹ï¼Œæˆ‘åªèƒ½åšå‡ºé‚£æ ·çš„å†³ç­–ã€‚

  ç°åœ¨å·²ç»è¿‡å»ä¸‰ä¸ªæœˆäº†ï¼Œç»è¿‡è¿™ä¸‰ä¸ªæœˆçš„æ¢ç´¢å’Œå­¦ä¹ ï¼Œæˆ‘ç°åœ¨çš„è®¤çŸ¥å’ŒæŠ€èƒ½åˆæœ‰äº†ä¸€äº›æå‡ï¼Œå¦‚æœä¸‹æ¬¡å†æœ‰ç±»ä¼¼çš„æœºä¼šï¼Œæˆ‘æƒ³ï¼Œæˆ‘åº”è¯¥ä¼šåšçš„æ›´å¥½ã€‚è¿™ä¹Ÿæ˜¯RHERé‡Œé¢ï¼Œè‡ªæˆ‘å¼•å¯¼æŒç»­å¼ºåŒ–å­¦ä¹  çš„ä¸€ä¸ªä½“ç°ã€‚
  
  å†å›åˆ°æœ€åˆçš„ç›®æ ‡ï¼Œ17å¹´çš„æ—¶å€™ï¼Œæˆ‘å°±æƒ³ç€ï¼Œèƒ½å¤ŸæŠŠæœ€æ–°çš„è‹±æ–‡å­¦æœ¯æˆæœï¼Œç¿»è¯‘æˆä¸­æ–‡ã€‚
  
  ç°åœ¨æ˜¯æˆ‘è·ç¦»è¿™ä¸ªç›®æ ‡æœ€è¿‘çš„ä¸€æ¬¡ã€‚

  æˆ‘æœ‰äº†ä¸€ä¸ªæ¯”è¾ƒç«çš„å¼€æºé¡¹ç›®ï¼Œè®©æˆ‘æœ‰æœºä¼šè®¤è¯†æ›´å¤šå‰å®³çš„äººï¼Œä¹Ÿä¼šæœ‰æ›´å¤šçš„äººè®¤å¯æˆ‘ï¼Œæ„¿æ„å¸®åŠ©æˆ‘ã€‚

  æ¯”å¦‚è¯´å­¦æœ¯ç‰ˆGPTçš„ä½œè€…ï¼Œä»–åšäº†ä¸€ä¸ªarXivçš„å…¨æ–‡ç¿»è¯‘çš„åŠŸèƒ½ã€‚

  æˆ‘ç¬¬ä¸€æ¬¡çœ‹åˆ°è¿™ä¸ªåŠŸèƒ½çš„æ—¶å€™ï¼Œæˆ‘å°±è¯´ï¼Œä»–åˆ›å»ºäº†ä¸€ä¸ªå­¦æœ¯â€œå·´åˆ«å¡”â€ã€‚

  æˆ‘çŸ¥é“ï¼Œ17å¹´çš„ç›®æ ‡ï¼Œå¤§æ¦‚æ˜¯å¯ä»¥å¤Ÿçš„ç€äº†ï¼Œå¦‚æœæŠŠé‚£ä¸ªç›®æ ‡å½“æˆæ˜¯desired goalçš„è¯ï¼Œç°åœ¨åº”è¯¥å·²ç»æ¥è¿‘æœ€åä¸€ä¸ªstageäº†ã€‚

  åœ¨å¾—åˆ°qingxuçš„æ”¯æŒä¸‹ï¼Œä»¥åŠé»„è€å¸ˆçš„å¸®åŠ©ä¸‹ï¼Œæˆ‘ä»¬ç»ˆäºå°†è¿™ä¸ªåŠŸèƒ½ä¸Šçº¿åˆ°äº†chatpaper.orgä¸Šã€‚

  ç°åœ¨è·ç¦»æˆ‘ä»¬æœ€åçš„ç›®æ ‡ï¼ŒæŠŠæ‰€æœ‰çš„ä¼˜è´¨è®ºæ–‡ç¿»è¯‘å±•ç¤ºåˆ°é¦–é¡µä¸Šï¼Œå½¢æˆä¸€ä¸ªéå¸¸å¥½çš„æ–‡çŒ®é˜…è¯»ç¤¾åŒºï¼Œè¿˜æœ‰ä¸€å®šçš„è·ç¦»ã€‚

  ä½†æ˜¯æˆ‘ç›¸ä¿¡ï¼Œæˆ‘ä»¬æ­£åœ¨step by stepçš„å»å®ç°å®ƒã€‚

  æˆ‘ç›¸ä¿¡æˆ‘ä»¬ç°åœ¨åšçš„è¿™é¡¹å·¥ä½œæ˜¯æœ‰ä»·å€¼çš„ï¼Œä¸–ç•Œä¸Šé‚£ä¹ˆå¤šéè‹±è¯­æ¯è¯­çš„å·¥ç¨‹å¸ˆã€ä½å¹´çº§çš„å­¦ç”Ÿã€å…¶ä»–é¢†åŸŸçš„ç ”ç©¶è€…ï¼Œä»–ä»¬å¦‚æœå¯ä»¥ç”¨æ¯è¯­å»é˜…è¯»æœ€æ–°çš„ç§‘æŠ€è®ºæ–‡æ—¶ï¼Œæ˜¯å¯ä»¥æå¤§çš„æ‹“å®½å¤§å®¶çš„è®¤çŸ¥çš„ã€‚
  
  è€Œè¯­è¨€è¿™ä¸ªé—¨æ§›å¦‚æœå­˜åœ¨çš„è¯ï¼Œåœ¨å®ç°{å­¦ä¹ æœ€æ–°ç§‘æŠ€è¿›å±•}ç›®æ ‡çš„è¿‡ç¨‹ä¸­ï¼Œå‡­ç©ºå¤šäº†ä¸€ä¸ªgapï¼Œæˆ–è€…ç”¨RHERä¸­çš„æ¦‚å¿µï¼Œå°±æ˜¯å¤šäº†ä¸€ä¸ªå­¦ä¹ è‹±è¯­çš„stageï¼Œå¦åˆ™æ ¹æœ¬æ— æ³•å®Œæˆè¿™ä¸ªä»»åŠ¡ã€‚

  RHERè§£å†³çš„æ˜¯RL agentåœ¨å¤æ‚åºåˆ—ä»»åŠ¡ä¸­æ¢ç´¢å’Œå­¦ä¹ æ•ˆç‡çš„é—®é¢˜ï¼Œæˆ‘ä¸€ç›´å¸Œæœ›èƒ½å°†è¿™ä¸ªå·¥ä½œæ‹“å±•åˆ°å…¶ä»–é¢†åŸŸã€‚

  å‰æ®µæ—¶é—´æœ‰åŒå­¦å°†è¿™ä¸ªå·¥ä½œæ‹“å±•åˆ°äº†Model-basedæ¶æ„ä¸­ï¼Œå¯æƒœå› ä¸ºå†™ä½œåŸå› ï¼Œç¬¬ä¸€æ¬¡æŠ•ç¨¿è¢«æ‹’äº†ã€‚

  æˆ‘ä¹Ÿæƒ³è¿‡å…¶ä»–çš„é¢†åŸŸï¼Œæ¯”å¦‚å¤šæ™ºèƒ½ä½“çš„åˆä½œæ¢ç´¢ï¼Œä½†ç›®å‰çš„å¼ºåŒ–ä»¿çœŸä»»åŠ¡éƒ½å¾ˆéš¾åˆ‡åˆ†å­ä»»åŠ¡å’Œç¦»æ•£çš„ä»»åŠ¡é˜¶æ®µï¼Œé™¤äº†ç‰©ä½“æ“ä½œä»»åŠ¡ä¹‹å¤–ï¼Œä¹Ÿå°±æˆ‘çš„ä¸–ç•Œæ¯”è¾ƒåˆé€‚ï¼Œä½†æš‚æ—¶æˆ‘å¯¹æˆ‘çš„ä¸–ç•Œçš„ç¯å¢ƒè¿˜ä¸ç†Ÿæ‚‰ï¼Œå…¶ä»–æ„Ÿå…´è¶£çš„åŒå­¦å¯ä»¥è¯•è¯•è¿™ä¸ªç»“åˆã€‚

  ç»è¿‡è¿™ä¹ˆé•¿æ—¶é—´çš„åæ€ï¼Œæˆ‘å‘ç°ï¼Œæœ€ç¬¦åˆçš„è¿˜æ˜¯æˆ‘ä»¬äººç±»çš„æ—¥å¸¸ä»»åŠ¡ï¼Œå¾ˆå¤šä»»åŠ¡éƒ½æ˜¯å¤æ‚åºåˆ—ä»»åŠ¡ï¼Œå‰ç½®ä»»åŠ¡æ²¡å®Œæˆï¼Œæ˜¯æ— æ³•è§£å†³æœ€ç»ˆç›®æ ‡çš„ã€‚è€Œäººç±»åœºæ™¯çš„ä¸€äº›ä»»åŠ¡ï¼Œè®©ä¼ ç»Ÿçš„å¼ºåŒ–å»å®Œæˆï¼Œå¯èƒ½æ¯”è¾ƒå›°éš¾ï¼Œåé¢éœ€è¦è®©LLMç»“åˆè‡ªæˆ‘å¼•å¯¼çš„ä¸€äº›æ¦‚å¿µï¼Œå»å®ç°æ‰æ¯”è¾ƒåˆé€‚ã€‚

  PhDæœ¬æ„æ˜¯å“²å­¦åšå£«ï¼Œåº•å±‚çš„æ–¹æ³•è®ºç¡®å®æ˜¯æœ‰å¼‚æ›²åŒå·¥ä¹‹å¦™ã€‚æˆ‘ç°åœ¨éå¸¸å¸Œæœ›RHERè¿™ç¯‡å·¥ä½œèƒ½è¢«é¡ºåˆ©æ¥å—ï¼Œè¿™æ ·æˆ‘å°±å¯ä»¥å†™æˆ‘çš„åšå£«å¤§è®ºæ–‡äº†ï¼Œæˆ‘æƒ³æˆ‘çš„åšå£«è®ºæ–‡åº”è¯¥æ˜¯å¯¹ç¤¾åŒºæœ‰å¸®åŠ©çš„ã€‚
  
  GPT4çš„å‡ºç°ï¼Œä¸€åº¦è®©æˆ‘æ„Ÿåˆ°ç„¦è™‘å’Œå¯¹äººç±»æœªæ¥æ„Ÿåˆ°æ‹…å¿§ï¼Œæˆ‘æ‹…å¿ƒä¸€ä¸ªè®¤çŸ¥å’Œå†³ç­–èƒ½åŠ›è¶…è¿‡å¤šæ•°äººç±»çš„æ¨¡å‹çš„å‡ºç°ï¼Œä¼šè®©å¾ˆå¤šäººæˆä¸ºGPTçš„é™„åº¸ï¼Œå¬ä»GPTçš„å†³ç­–ã€‚

  ç»è¿‡è¿™ä¸‰ä¸ªæœˆçš„äº¤äº’å’Œåé¦ˆï¼Œæˆ‘è¿˜æ˜¯åšæŒå½“æ—¶å¼€æºChatPaperæ—¶çš„åŠªåŠ›ï¼Œæˆ‘å¸Œæœ›æˆ‘å’Œä¼™ä¼´ä»¬çš„å·¥ä½œï¼Œèƒ½å¤Ÿå¸®åŠ©æˆ‘ä»¬äººç±»è‡ªå·±åœ¨AIå¿«é€Ÿè¿­ä»£çš„æ—¶ä»£ä¸­ï¼Œä¹Ÿèƒ½å’ŒAIä¸€èµ·è¿›åŒ–ã€‚
  
  ä¸ç®¡æ˜¯AI4Scienceï¼Œè¿˜æ˜¯å…¶ä»–ï¼Œæˆ‘å¸Œæœ›AI4Humanã€‚

  ä¸ºäº†å®ç°è¿™æ ·çš„ç›®æ ‡ï¼Œåé¢è¿˜æœ‰å¾ˆå¤šä¸œè¥¿è¦åšï¼Œè¿˜æœ‰å¾ˆå¤šå‘éœ€è¦æ¢ç´¢ã€‚

  å¥½åœ¨æ¯ä¸€æ¬¡é‡åˆ°æˆ‘ä¸ä¼šçš„é—®é¢˜ï¼Œéƒ½èƒ½å¾—åˆ°å¯¹åº”domainçš„expert agentçš„guidanceï¼Œè®©æˆ‘å¾—ä»¥å¿«é€Ÿè§£å†³é—®é¢˜ä¸”å­¦ä¼šæ–°çš„æŠ€èƒ½ï¼Œä»è€Œå»å®Œæˆæ›´åŠ æœ‰æŒ‘æˆ˜æ€§çš„å·¥ä½œã€‚
  
  å¸Œæœ›åœ¨è¿™ä¸ªæœ‰é™çš„ï¼Œä»…æœ‰ä¸€æ¬¡çš„ç”Ÿå‘½ä¸­ï¼Œåœ¨è¿™ä¸ªæå…·å˜åŒ–çš„æ—¶ä»£ä¸­ï¼Œåšå‡ºç‚¹å¾®å°çš„è´¡çŒ®ï¼Œå¸Œæœ›åœ¨æˆ‘æ“…é•¿çš„é¢†åŸŸï¼Œå¯ä»¥å¸®åŠ©åˆ°å…¶ä»–çš„learning agentã€‚

  å¤ªæ™šäº†ï¼Œæ€è·¯é›¶æ•£ï¼Œæƒ³åˆ°ä»€ä¹ˆè¯´ä»€ä¹ˆï¼Œå…ˆå‘å‡ºæ¥ï¼Œå’Œå„ä½å…±å‹‰ã€‚
  
  
  
  
  
</details>

We express our gratitude for the expert guidance! 

With the advice of the expert, we evaluated the cost of our solution in detail. After all, there is no free lunch, but by comparison, our solution only needs to pay a small price, which can greatly reduce the learning time of the whole task. It can be easily calculated from the table that in the multi-object tasks, the memory and computation time have a simple linear relationship with the number of objects, and the linear increase coefficient is very low.

![image](https://github.com/kaixindelele/RHER/assets/28528386/2fb27eb7-12db-4f9f-8679-24c33dada3dc)

In addition, with the advice of experts, we investigated more than 40 paper on multi-task reinforcement learning in the past five years, and found that our zero-padding encoding also has certain promotion value in the field of multi-task reinforcement learning, especially in the field of robot manipulation tasks, dynamic objects and goal tasks. RHER's framework really suits the new backbone in the field of robot manipulation tasks.

Even in the era of LLMs, the idea of self-guided exploration can also enable LLms-based methods to achieve a stable exploration. We're trying it out, so stay tuned!






ğŸ’¥ğŸ’¥ğŸ’¥<strong> 4/12/2023. RHER vs SOTA HER-based method, EBP, based on [Energy-Based Hindsight Experience Prioritization](https://github.com/ruizhaogit/EnergyBasedPrioritization)
</details>
</strong>

![RHER_SOTA](https://user-images.githubusercontent.com/28528386/231344134-d5a46362-afb8-42c0-8e18-ce8c16ba8960.png)

Under the more realistic single CPU core setting, although the EBP algorithm has achieved a great improvement in sample efficiency compared with HER, it still has obvious disadvantages compared with our RHER algorithm due to the lack of self-guided exploration. All experiments were conducted with the same five random seeds (1000-5000) and the hyperparameter clip_energy of EBP is 0.5.

## Natter 2023-02-18-10am:
Yesterday, I review the Reincarnating RL (https://agarwl.github.io/reincarnating_rl/), and found that Jump Start RL (JSRL) has the state-distribution problem when using the guide-policy, while our Self-Guided Exploration Strategy (SGES) does not. Because JSRL uses the guide-policy with a certain trajectory, then transfers to learning-policy, this combination has the state-distribution problem naturally~

Our SGES mixes guide-policy and learning-policy with the same probability so that they have the same state-distribution~

è¿™ç§è‡ªæˆ‘å¼•å¯¼çš„æ–¹æ¡ˆï¼Œåœ¨å¤§è¯­è¨€æ¨¡å‹ç­–ç•¥ä¸­ä»ç„¶å¥½ç”¨~

This self-guided approach is still useful in large language model.



## ä¸­æ–‡ç‰ˆç¢ç¢å¿µï¼š2023-02-18-10am
<details><summary><code>æŸ¥çœ‹ä¸­æ–‡ç‰ˆç¢ç¢å¿µ</code></summary>
æœ€è¿‘çœ‹äº†ã€ŠDichotomy of Control: Separating What You Can Control from What You Cannotã€‹ï¼Œå…¶å®æˆ‘æ–‡ä¸­æ‰€æè¿°çš„ä¸€è‡´éè´Ÿå¥–åŠ±ï¼Œä¹Ÿæ˜¯å…¶ä¸­çš„ä¸€ç§æƒ…å†µï¼Œåªä¸è¿‡åºåˆ—é—®é¢˜æ“ä½œä»»åŠ¡ï¼Œå¦‚æœæŒ‰ç…§è¿™ä¸ªæè¿°ï¼Œå°±å¤ªå¤§äº†ï¼Œè®ºæ–‡ä¸­ï¼Œè¿˜æ˜¯ä»¥â€œå¤¹çˆªæ— æ³•æ”¹å˜ç‰©ä½“çš„ä½ç½®ï¼Œè€Œå¯¼è‡´ç”¨HERåæ€åï¼Œä¼šå­˜åœ¨ éšå¼çš„éè´Ÿç¨€ç–å¥–åŠ±ä¸ºåˆ‡å…¥ç‚¹â€ æ¯”è¾ƒåˆé€‚ã€‚å€Ÿç€è¿™ä¸ªæœºä¼šï¼Œæˆ‘æƒ³åˆ†äº«ä¸€ä¸‹è®ºæ–‡ä¹‹å¤–çš„æ„Ÿè§¦ã€‚

å¯¹äºæ™ºèƒ½ä½“çš„æ¢ç´¢å’Œå­¦ä¹ ï¼Œå°¤å…¶æ˜¯ç¨€ç–å¥–åŠ±ä¸‹çš„æ•ˆç‡é—®é¢˜ï¼Œæˆ‘ä»18å¹´åº•å°±å¼€å§‹å°è¯•äº†ï¼Œåˆ°ç°åœ¨å·²ç»å››å¹´å¤šäº†ï¼Œæˆ‘å‘ç°æˆ‘å¯¹å¼ºåŒ–å­¦ä¹ çš„researchï¼Œä¹Ÿç®—æ˜¯ä¸€ç§é•¿åºåˆ—ç¨€ç–å¥–åŠ±è¿‡ç¨‹ã€‚

åœ¨æˆ‘é•¿æ—¶é—´æ— æ³•åšå‡ºé¡¶ä¼šé¡¶åˆŠçš„å·¥ä½œçš„æ—¶å€™ï¼Œæˆ‘ä¹Ÿåœ¨åæ€ï¼Œä¸ºä»€ä¹ˆæˆ‘ä¼šèµ¶ä¸ªæ—©é›†ï¼Œå´è¿å£çƒ­ä¹çš„éƒ½åƒä¸ä¸Šï¼Ÿä¸ºä½•æˆ‘çš„å­¦ä¹ æ•ˆç‡å¦‚æ­¤ä½ä¸‹ï¼Ÿ

ç°åœ¨å›æƒ³èµ·æ¥ï¼Œä¸€ç›´é™·å…¥å±€éƒ¨æœ€â€œä¼˜â€çš„æˆ‘ï¼Œç›´åˆ°21å¹´ï¼Œæ‰æœ‰ç‚¹å˜åŒ–ã€‚

21å¹´åº•ï¼Œæˆ‘æ›¾ç»èŠ±äº†å¾ˆé•¿çš„æ—¶é—´æ¥å¤ç°è‘—åçš„HERç®—æ³•ï¼Œè¿™ç¯‡å·¥ä½œçš„æ–¹æ¡ˆæ˜¯å¦‚æ­¤çš„ç®€æ´ä¼˜é›…ï¼Œä»¥è‡³äºåé¢å‡ºç°äº†å¿«ä¸¤åƒçš„å¼•ç”¨ã€‚å…ˆè§£é‡Šä¸‹HERç®—æ³•ï¼ŒHERç®—æ³•æœ¬è´¨ä¸Šæ˜¯ä¿®æ”¹ç›®æ ‡ï¼Œè®©æ™ºèƒ½ä½“æœ‰ä¸€ä¸ªç±»ä¼¼çš„â€œå®‰æ…°å¥–â€ï¼šè™½ç„¶å®ƒæ²¡å®Œæˆç»™å®šçš„ä»»åŠ¡ï¼Œä½†å¦‚æœå½“åˆè®¾å®šçš„ç›®æ ‡ï¼Œå°±æ˜¯å®ƒåˆšæ‰å®Œæˆçš„ï¼Œé‚£å®ƒå²‚ä¸æ˜¯å¯ä»¥è®¤ä¸ºå·²ç»æˆåŠŸäº†ï¼Ÿ

è¿™å¬èµ·æ¥æœ‰ç‚¹åºŸè¯æ–‡å­¦ï¼Œä½†å®é™…ä¸Šï¼Œå¦‚æœå®ƒå®Œæˆä»»åŠ¡çš„æŸäº›éƒ¨åˆ†ï¼Œä¸‹æ¬¡ä»»åŠ¡çœŸçš„ç»™å®šäº†è¿™ä¸ªç›®æ ‡ï¼Œå®ƒä¸Šæ¬¡å­¦åˆ°çš„çŸ¥è¯†æ˜¯çœŸçš„èƒ½ç”¨ä¸Šï¼

æ‰€ä»¥å›è¿‡æ¥çœ‹ï¼Œæˆ‘ä¹‹å‰æ¢ç´¢çš„é‚£äº›å¤±è´¥çš„ç»å†ä¹Ÿæ˜¯æœ‰ç”¨ã€‚æ¯”å¦‚è¯´ï¼Œæˆ‘èŠ±äº†2ä¸ªæœˆæ—¶é—´å¤ç°OpenAI Baselineç‰ˆæœ¬çš„HERï¼Œå› ä¸ºæˆ‘ä¸€ç›´æ— æ³•ç”¨Pytorchå¤ç°å‡ºå’Œå®ƒä¸€æ ·çš„æ€§èƒ½ï¼Œæ‰€ä»¥æˆ‘å‡ ä¹å°è¯•äº†æ¯ä¸€ä¸ªè¶…å‚æ•°å’Œè®¾ç½®ã€‚åœ¨å°è¯•æ‰¹é‡è°ƒå‚çš„è¿‡ç¨‹ä¸­ï¼Œå› ä¸ºæ‰‹åŠ¨å¯åŠ¨ç¨‹åºçš„æ•ˆç‡å¤ªä½äº†ï¼Œå› æ­¤å°†spinningupçš„MPIæ•™ç¨‹æ”¹æˆäº†èƒ½ç½‘æ ¼æœç´¢è¶…å‚çš„æ¨¡å¼ï¼Œæ•™ç¨‹å¦‚ä¸‹ï¼šhttps://blog.csdn.net/hehedadaq/article/details/114685906ã€‚

è™½ç„¶æˆ‘åœ¨è¿™ä¹ˆé•¿çš„æ¢ç´¢è¿‡ç¨‹ä¸­ï¼Œæ²¡æœ‰åšå‡ºæ–°é¢–çš„å·¥ä½œï¼Œä½†æ˜¯åœ¨è¿‡ç¨‹ä¸­ï¼Œæˆ‘å‘ç°äº†ä¸€ä¸ªæ–°é—®é¢˜ï¼Œå°±æ˜¯æ¸²æŸ“è¿‡ç¨‹ä¸­ï¼Œæˆ‘å‘ç°ï¼Œæ¯æ¬¡FetchPushä»»åŠ¡åœ¨æ¢ç´¢å‰æœŸï¼Œéƒ½ä¼šå‡ºç°æœºæ¢°è‡‚ä¸çŸ¥é“è´´è¿‘ç‰©å—çš„æƒ…å†µï¼Œæˆ‘å½“æ—¶æœ€ç›´è§‚çš„æƒ³æ³•å°±æ˜¯ï¼Œè¿™ä¸ªè‚¯å®šä¼šå½±å“æ¢ç´¢æ•ˆç‡ï¼okï¼Œæ‰¾åˆ°äº†é—®é¢˜ï¼Œæˆ‘å¼€å§‹ç¬¬ä¸€æ­¥å°è¯•ï¼Œå°±æ˜¯åœ¨æ¢ç´¢è¿‡ç¨‹ä¸­ï¼Œè®©æ™ºèƒ½ä½“å…ˆå­¦æ¥è¿‘ï¼Œå†å­¦æ“ä½œï¼ˆç­‰æˆ‘åšå®Œæ•´ä¸ªå®éªŒï¼Œå¼€å§‹å†™æ–‡ç« ï¼Œæ–‡çŒ®è°ƒç ”çš„æ—¶å€™ï¼Œæ‰çŸ¥é“ï¼Œè¿™æ˜¯SHERè¿™ç¯‡æ–‡ç« çš„å¤„ç†æ–¹æ¡ˆï¼‰ï¼Œç„¶åå‘ç°æ•ˆæœæ¯”ä¸ä¸Šï¼Œæ¥è¿‘å’Œæ“ä½œä¸€èµ·å­¦ï¼Œæ¢ç´¢çš„æ—¶å€™æ··åˆæ¢ç´¢ã€‚ä¹Ÿå°±æ˜¯æˆ‘è‡ªå·±çš„RHERæ–¹æ¡ˆåŸå‹ã€‚

è·³å‡ºè¿™ä¸ªå®éªŒçš„ç»†èŠ‚ï¼Œå¯¹äºæˆ‘è‡ªå·±æ¥è¯´ï¼Œå‰é¢å¯¹HERçš„æ¢ç´¢ï¼Œå¯¹HERçš„ç†è§£ï¼Œæ‰¹é‡è°ƒå‚çš„æŠ€èƒ½ï¼Œè®©æˆ‘åé¢å¤„ç†åŸç‰ˆRHERç®—æ³•ç›´æ¥æ¨å¹¿åˆ°å…¶ä»–å¤šç‰©ä½“æ“ä½œä»»åŠ¡æ— æ³•workæ—¶ï¼Œæœ‰äº†åŸºç¡€ï¼Œåœ¨æ–‡ç« ä¸­çš„ä½“ç°å°±æ˜¯ï¼Œæˆ‘åšäº†å…­ä¸ƒé¡¹è®¾ç½®çš„ä¿®æ”¹ï¼Œç»ˆäºå‘æŒ¥äº†RHERçš„æ½œåŠ›ï¼Œè¾¾åˆ°äº†æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ åœ¨å¤šç‰©ä½“æ“ä½œä»»åŠ¡ä¸Šçš„sota.

è¿‡å»çš„â€œå¤±è´¥â€ç»éªŒå’ŒæŠ€èƒ½ï¼Œæ˜¯ä¼šå¸®åŠ©åé¢çš„æˆé•¿çš„ï¼Œæ–‡ç« ä¸­æ˜¯è¿™æ ·ï¼Œæ–‡ç« å¤–ä¹Ÿæ˜¯ä¸€æ ·ã€‚

å› ä¸ºä»Šå¤©æ˜¯ä¸€ä¸ªå‘¨å…­çš„ä¸Šåˆï¼Œæ‰€ä»¥æˆ‘æœ‰æ—¶é—´å†æ¢³ç†ä¸‹ï¼Œå¯¹äºç¨€ç–å¥–åŠ±ä¸‹çš„å¤æ‚åºåˆ—ä»»åŠ¡ã€‚æœ‰å‡ ä¸ªç‚¹å¯ä»¥å¸®åŠ©æ™ºèƒ½ä½“å¿«é€Ÿå­¦ä¹ ï¼š

1. è®¾å®šä¸€ä¸ªå¾ªåºæ¸è¿›çš„ç›®æ ‡ï¼Œä¸€å¼€å§‹å°±å¥½é«˜éª›è¿œæ˜¯ä¸åˆç†çš„ã€‚æ–‡ä¸­çš„ä½“ç°å°±æ˜¯å¯¹ä»»åŠ¡çš„æ‹†è§£ï¼Œæ–‡å¤–çš„ä½“ç°å°±æ˜¯ç»™è‡ªå·±ä¹Ÿè®¾å®šä»»åŠ¡æ¸…å•ã€‚

2. å­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œè¦æ‰¾åˆ°å“ªäº›æ˜¯è‡ªå·±å¯æ§çš„ï¼Œå“ªäº›æ˜¯ç¯å¢ƒæˆ–è€…å…¶ä»–æ™ºèƒ½ä½“å¯¼è‡´çš„ã€‚å¯¹è‡ªå·±å¯æ§çš„ç»éªŒï¼Œä½ çš„å­¦ä¹ æ‰æœ‰æ•ˆæœã€‚æ–‡ä¸­çš„ä½“ç°å°±æ˜¯ï¼Œè¦é™ä½INNRæ¯”ä¾‹ï¼Œè®©å¤¹çˆªå°½å¯èƒ½çš„å½±å“ç‰©å—ï¼Œæ–‡å¤–çš„ä½“ç°å°±æ˜¯ï¼Œå¦‚æœä½ çš„è¡Œä¸ºæ”¹å˜ä¸äº†äº‹ä»¶çš„å‘ç”Ÿï¼Œé‚£ä¹ˆè¿™ä¸ªäº‹ä»¶å’Œä½ çš„å› æœæ€§å°±å¾ˆä½ï¼Œä½ çš„æ€»ç»“åæ€ï¼Œéƒ½æ˜¯ä½æ•ˆç‡çš„ã€‚

3. æ¢ç´¢çš„è¿‡ç¨‹ä¸­ï¼Œæœ‰è´µäººç›¸åŠ©ï¼Œæœ‰ä¸“å®¶å¯¼å¸ˆçš„æ‰‹æŠŠæ‰‹æŒ‡å¯¼ï¼Œæ˜¯å¿«é€Ÿé€šè¿‡å‰äººå·²ç»æ¢ç´¢æ˜ç™½çš„é¢†åŸŸçš„æ·å¾„ã€‚æ–‡ä¸­æ˜¯è®©å¤æ‚å­ä»»åŠ¡çš„æ¢ç´¢ï¼Œå—åˆ°å·²ç»å­¦ä¼šçš„ç®€å•å­ä»»åŠ¡çš„ç­–ç•¥çš„å¼•å¯¼ã€‚æ–‡å¤–çš„ä¹Ÿå’Œé‚£åç†è§£ï¼Œä½œä¸ºç ”ç©¶ç”Ÿçš„æˆ‘ï¼Œä¼šåœ¨å¯¼å¸ˆï¼Œå¸ˆå…„å¸ˆå§çš„æŒ‡å¯¼ä¸‹ï¼Œè¿›è¡Œç ”ç©¶çš„å…¥é—¨ï¼Œä¼šé˜…è¯»å„ä½å‰è¾ˆä¸“å®¶çš„è®ºæ–‡å’Œä»£ç ä»¥åŠåšå®¢ã€‚ç„¶åå¼€å§‹æˆ‘è‡ªå·±çš„æ¢ç´¢ï¼Œæ€»ç»“å’Œåˆ†äº«ã€‚

4. è¿˜æœ‰ä¸€ä¸ªç‚¹ï¼Œæ˜¯è¿™ç¯‡æ–‡ç« æ²¡æœ‰è®¨è®ºçš„ï¼Œæ˜¯æˆ‘ä¸‹ä¸€ç¯‡æ–‡ç« çš„å†…å®¹ï¼Œç­‰æœ‰æœºä¼šäº†å†å’Œå¤§å®¶åˆ†äº«ã€‚

è‡³äºç¬¬ä¸‰ç‚¹ï¼Œæˆ‘å†è®²ä¸€ä¸‹ï¼Œæ–‡ä¸­æåˆ°äº†ä¸€ä¸ªè‡ªæˆ‘å¼•å¯¼æ¢ç´¢ã€‚å› ä¸ºæ–‡ç« æ¶‰åŠçš„å†…å®¹å¤ªå¤šäº†ï¼Œæˆ‘è‡ªå·±çš„å†™ä½œæŠ€èƒ½è¿˜æ²¡æœ‰è¾¾åˆ°é©¾è½»å°±ç†Ÿçš„ç¨‹åº¦ï¼Œæ‰€ä»¥æœ‰æ®µå†…å®¹ï¼Œæ²¡èƒ½ä¼˜é›…çš„åŠ åˆ°introductioné‡Œé¢ã€‚
è‡ªæˆ‘å¼•å¯¼ï¼Œéœ€è¦å›ç­”ä¸¤ä¸ªé—®é¢˜ï¼Œä¸€ä¸ªæ˜¯å¦‚ä½•è¯„ä»·ä¸€ä¸ªç­–ç•¥æ˜¯ä¸“å®¶ç­–ç•¥ï¼Œå¦å¤–ä¸€ä¸ªæ˜¯å¦‚ä½•åˆ©ç”¨å¥½ä¸“å®¶ç­–ç•¥ï¼Ÿ
ç°å®ç”Ÿæ´»ä¸­ï¼Œä¸“å®¶ï¼Œæ˜¯éœ€è¦æœ‰å®¢è§‚è¯„ä»·æŒ‡æ ‡çš„ï¼ŒèŒä¸šï¼Œå­¦å†ï¼Œä¸“ä¸šï¼Œæ–‡ç« ï¼Œä¸“åˆ©ï¼Œçªå‡ºçš„é¡¹ç›®ç»å†ç­‰ç­‰ï¼Œéœ€è¦æœ‰æ˜ç¡®çš„è¯„ä»·ä½“ç³»ï¼Œæ¥è¡¡é‡æŸä¸ªäººåœ¨æŸä¸ªç‰¹å®šçš„é¢†åŸŸæ˜¯å¦æ˜¯ä¸“å®¶ã€‚
å¦å¤–ä¸€ä¸ªå°±æ˜¯å¦‚ä½•æ›´å¥½çš„åˆ©ç”¨ä¸“å®¶çš„æŒ‡å¯¼ï¼Ÿå…¶å®åˆ‡æ¢åˆ°äººçš„è§†è§’ï¼Œåº”è¯¥è¯´â€œä¸“å®¶å¦‚ä½•æŒ‡å¯¼ï¼Œæ‰ä¼šæ›´åŠ é«˜æ•ˆâ€æ›´ä¸ºç¤¼è²Œã€‚
ççœ¼çœ‹ä¸“å®¶æ“ä½œï¼Œæ˜¯å¾ˆéš¾å­¦ä¼šçš„ï¼›ä¸“å®¶çœ‹ç€ä½ çš„æ“ä½œï¼Œå¶å°”ç‚¹æ‹¨ä½ ä¸€ä¸‹ï¼Œä½ å­¦ä¹ æ•ˆç‡è‚¯å®šä¼šå˜é«˜ï¼›ä½†æœ€å¥½çš„è¿˜æ˜¯â€œæ‰‹æŠŠæ‰‹â€æŒ‡å¯¼ï¼Œæ—¢èƒ½æœ‰è‡ªå·±çš„æ¢ç´¢ï¼Œåœ¨èµ°åçš„æ—¶å€™ï¼Œåˆèƒ½æœ‰äººåŠæ—¶è§„æ­£ã€‚

è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œå¯¹äºRHERæ–‡ç« æ¥è¯´ï¼Œæˆ‘ä»¬è®¨è®ºçš„æ˜¯åºåˆ—ç‰©ä½“æ“ä½œä»»åŠ¡ï¼Œå®ƒè™½ç„¶æœ¬èº«æ˜¯ä¸€ç±»å¸¸è§ä¸”é€šç”¨çš„ä»»åŠ¡ï¼Œä½†æ˜¯å®ƒè¿˜æ˜¯å…·æœ‰ç‰¹æ®Šæ€§çš„ï¼Œå®ƒå¯ä»¥éå¸¸å¥½çš„åˆ‡åˆ†æˆå¤šä¸ªé˜¶æ®µã€‚
å› æ­¤æˆ‘ä»¬å¯ä»¥å¾ˆå¥½çš„è¯„ä¼°æ¯ä¸ªå­ç­–ç•¥çš„æˆåŠŸç‡ï¼Œæ¥è¡¡é‡â€œä¸“ä¸šç¨‹åº¦â€ã€‚
ç¬¬äºŒä¸ªé—®é¢˜å°±æ˜¯ï¼Œæˆ‘ä»¬ç”¨çš„æ˜¯æ··åˆæ¢ç´¢ï¼Œæ—¢èƒ½ä¿è¯æ•ˆç‡ï¼Œåˆèƒ½ä¿è¯ç­–ç•¥ä¸ä¼šå‡ºç°ç¦»çº¿æ•°æ®åˆ†å¸ƒä¸åŒ¹é…çš„æƒ…å†µã€‚

å› æ­¤ï¼Œæ•´ä¸ªRHERç®—æ³•ï¼Œå¯ä»¥è·å¾—æé«˜çš„æ ·æœ¬æ•ˆç‡ï¼Œå®Œå…¨å¯ä»¥ä½œä¸ºå¸¸è§åºåˆ—æ“ä½œä»»åŠ¡çš„éª¨å¹²ç®—æ³•ï¼Œå…¶ä¸­çš„è‡ªæˆ‘å¼•å¯¼æ¢ç´¢ï¼Œä¹Ÿå¯ä»¥æ¨å¹¿åˆ°å¤šæ™ºèƒ½ä½“æ¢ç´¢å’Œåˆ†å±‚å¼ºåŒ–ä¸­~

æœ€åï¼Œæ„Ÿè°¢å„ä½ä¸“å®¶æ— å¿çš„æ‰¹è¯„å’ŒæŒ‡å¯¼ï¼Œä»¥ä¾¿è®©è¿™ç¯‡å·¥ä½œæœ‰æ›´å¥½çš„å‘ˆç°ï¼Œè®©æˆ‘è‡ªå·±ä¹Ÿèƒ½å¤Ÿæ›´å¥½çš„æˆé•¿~~~
</details>

<details><summary><code>English version</code></summary>
  I recently read "Dichotomy of Control: Separating What You Can Control from What You Cannot," and the consistent non-negative rewards I described in the text are actually one of the cases. However, in the sequence problem operation task, if it's described this way, it would be too broad. In the paper, it's more appropriate to use the "implicit non-negative sparse reward after using HER for reflection due to the gripper's inability to change the object's position" as the starting point. Taking this opportunity, I would like to share some thoughts outside of the paper.

I have been trying to explore and learn about agents, especially the efficiency of sparse rewards, since the end of 2018. It's been more than four years now, and I found that my research on reinforcement learning can also be considered a long sequence sparse reward process.

When I was unable to produce top-tier conference and journal papers for a long time, I reflected on why I was up early but couldn't even eat a hot meal. Why was my learning efficiency so low?

Looking back now, I, who was stuck in the local optimum, did not see any changes until 2021.

At the end of 2021, I spent a long time reproducing the famous HER algorithm. The approach of this work is so simple and elegant that it has received nearly 2,000 citations. To explain the HER algorithm, it essentially modifies the goal to give the agent a sort of "consolation prize": Although it did not complete the given task, if the original goal was what it just accomplished, can it be considered successful?

This may sound like nonsense literature, but in fact, if it completes some parts of the task, the next time the task is truly given this goal, the knowledge it learned last time can really be used!

So looking back, my previous explorations and failures were also useful. For example, I spent 2 months reproducing the OpenAI Baseline version of HER, as I couldn't reproduce the same performance with Pytorch. I tried almost every hyperparameter and setting. In the process of batch tuning, since manually launching the program was too inefficient, I changed the spinningup's MPI tutorial into a grid search hyperparameter mode. The tutorial is here: https://blog.csdn.net/hehedadaq/article/details/114685906.

Although I didn't produce any novel work during this long exploration process, I discovered a new problem. In the rendering process, I found that every time the FetchPush task was in the early stages of exploration, the robotic arm didn't know how to get close to the object. My most intuitive idea at the time was that this would definitely affect the efficiency of exploration! Ok, I found the problem, and my first attempt was to let the agent learn to approach before learning to operate (when I finished the entire experiment and started writing the article, I realized during the literature review that this was the approach of the SHER paper). Then I found that the effect was not as good as learning to approach and operate together, and I mixed exploration during the exploration process. This is the prototype of my RHER scheme.

Stepping out of the details of this experiment, for me, the previous exploration of HER, the understanding of HER, and the skills of batch tuning laid the foundation for me to later deal with the original RHER algorithm when it couldn't be directly extended to other multi-object operation tasks. In the article, this is reflected in the fact that I made six or seven setting modifications, finally unleashing the potential of RHER and achieving the state of the art in model-free reinforcement learning for multi-object operation tasks.

Past "failed" experiences and skills will help with future growth, both in and out of the article.

Since today is a Saturday morning, I have some time to sort through the complex sequential tasks under sparse rewards. There are a few points that can help an agent learn quickly:

Set progressive goals; it is unreasonable to aim too high from the start. This is reflected in the article by breaking down tasks, and outside of the article by setting task lists for oneself.

During the learning process, identify what is within your control and what is caused by the environment or other agents. Effective learning occurs when focusing on controllable experiences. In the article, this is represented by lowering the INNR ratio, allowing the gripper to influence the block as much as possible. Outside the article, if your actions cannot change the outcome of an event, then the causality between you and the event is low, and any reflection or conclusions drawn will be inefficient.

In the exploration process, having the help of mentors and experts is a shortcut to quickly navigate through areas that others have already explored. In the article, the exploration of complex subtasks is guided by the strategies of simpler subtasks that have already been learned. Outside the article, as a graduate student, I will receive guidance from mentors, senior students, and read papers, code, and blogs from experts in the field before starting my own exploration, summarization, and sharing.

There is another point not discussed in this article, which will be the content of my next article. I'll share it with everyone when the opportunity arises.

Regarding the third point, the article mentioned self-guided exploration. Because the content of the article is vast and my writing skills have not yet reached a proficient level, I couldn't elegantly include this in the introduction. Self-guidance needs to answer two questions: how to evaluate if a strategy is an expert strategy, and how to make good use of expert strategies? In real life, experts need objective evaluation criteria such as occupation, education, expertise, publications, patents, outstanding project experience, and so on. Another question is how to make better use of expert guidance? From a human perspective, it would be more polite to say "How can experts guide more efficiently?" Watching experts work is difficult to learn from; having an expert watch your actions and occasionally give pointers can improve your learning efficiency. However, the best approach is hands-on guidance, allowing for personal exploration while having someone to correct you when you deviate.

These two questions, for the RHER article, are about sequential object manipulation tasks. Although they are a common and general category of tasks, they still have unique characteristics and can be well-divided into multiple stages. Thus, we can evaluate the success rate of each sub-strategy to determine their "expertise level." The second question is addressed by using a hybrid exploration approach that ensures efficiency while preventing policy mismatches due to offline data distribution.

Therefore, the RHER algorithm achieves extremely high sample efficiency and can serve as a backbone algorithm for common sequential manipulation tasks. Its self-guided exploration can also be extended to multi-agent exploration and hierarchical reinforcement learning.

Lastly, I'd like to thank all the experts for their invaluable criticism and guidance, which has allowed this work to be presented more effectively and helped me grow as well."
</details>

## 1. Abstract:
> Learning with sparse rewards remains a challenging problem in reinforcement learning (RL). Especially for sequential object manipulation tasks, the RL agent always receives negative rewards until completing all of the sub-tasks, which results in low exploration efficiency. To tackle the sample inefficiency for sparse reward sequential object manipulation tasks, we propose a novel self-guided continual RL framework, named Relay Hindsight Experience Replay (RHER). RHER decomposes the sequential task into several sub-tasks with increasing complexity and ensures that the simplest sub-task can be learned quickly by applying HER. Meanwhile, a multi-goal & multi-task network is designed to learn all sub-tasks simultaneously. In addition, a SelfGuided Exploration Strategy (SGES) is proposed to accelerate exploration. With SGES, the already learned sub-task policy will guide the agent to the states that are helpful to learn more complex sub-task with HER. Therefore, RHER can learn sparse reward sequential tasks efficiently stage by stage. The proposed RHER trains the agent in an end-to-end manner and is highly adaptable to avariousmanipulation tasks with sparse rewards. The experimental results demonstrate the superiority and high efficiency of RHER on a variety of single-object and multi-object manipulation tasks (e.g., ObstaclePush, DrawerBox, TStack, etc.). We perform a real robot experiment that agents learn how to accomplish a contact-rich push task from scratch. The results show that the success rate of the proposed method RHER reaches 10/10 with only 250 episodes.

## 2. Contributions:
(1) For common complex sequential object manipulation tasks with sparse rewards, this paper develops an elegant and sample efficient **self-guided continual RL framework**, RHER.

(2) To achieve self-guided exploration, we propose a **multi-goal & multi-task** network to learn multiple sub-tasks with different complexity simultaneously.

(3) The proposed RHER method is more sample-efficient than vanillaHER and other state-of-the-art methods, which are validated in the standard manipulation tasks from the OpenAI Gym. Further, to validate the versatility of RHER, we design eight sequential object manipulation tasks, including five complex multi-object tasks, which are available at this libary. The results show that the proposed RHER method consistently outperforms the vanilla-HER in terms of sample efficiency and performance.

(4) The proposed RHER learns a contact-rich task on a physical robot from scratch within 250 episodes in real world.


**I had release all codes for single-object tasks, if this paper is accepted, I will release the codes for multi-object tasks with the pytorch version immediately.**

-----

Although the mainstream tasks are soft robot and deformable object, my work provides an more effecient RL scheme for RL-Robotics community.

RHER is efficient and concise enough to be a new benchmark for the manipulation tasks with sparse rewards.

## 3. Suitable tasks:
Complex sequential object manipulation tasks, in which both objects (Num <= 3) and goals are within the workspace of the robot.

![RHER_multi_obj](https://user-images.githubusercontent.com/28528386/199898455-aa75683a-6803-4101-a48b-11425c924aae.png)

Fig1. Multi-object tasks graphs.


![Fig_multi_obj](https://user-images.githubusercontent.com/28528386/199915337-a5649596-fd22-40a4-a027-fed6ccb35342.png)

Fig2. Learning curve of multi-object tasks.

Unsuitable tasks:
Stroke tasks: Slide, Tennis.

-----

## 4. Motivation:
HER works for simple reach tasks, but faces low sample efficient for manipulation tasks.
![image](https://user-images.githubusercontent.com/28528386/200155407-c5461a1f-ef55-4f97-8537-bab87af11d8b.png)

Each epoch means 19 * 2 * 50 = 1900 episodes!

Reported in 'Multi-Goal Reinforcement Learning: Challenging Robotics Environments and Request for Research'

I found an implicit problem of HER:

## 5. HER introduces an implicit **non-negative** sparse reward problem for manipulation tasks

HER has an implicit **non-negative** sparse reward problem caused by indentical achieved goals! 

![HER_INNR_ac](https://user-images.githubusercontent.com/28528386/218956856-837d2a52-8a8b-44e0-a1d8-11747dc24422.png)




Fig. 3. Illustration of the difference of HER and RHER. (a) The problem of Identical Non-Negative Rewards (INNR) with HER. (b) The proposed RHER solves the INNR problem by Self-Guided Exploration Strategy (SGES). (c) The **surprising** results of comparation of RHER and HER in FetchPush (If our codes is not open source, it may seem a bit outrageous~ Today, I read the controversy of the corpus indexer of NIPS and rethink our results. There should be no bug in my project, because the efficiency of the real machine is really high~).

## 6. A diagram of RHER:
![RHER_overall](https://user-images.githubusercontent.com/28528386/218956892-6a720c84-cd56-4cbb-8864-bc6d0fe6d3c9.png)


Fig4. A diagram of RHER, of which the key components are shown in the yellow rectangles. This framework achieves self-guided exploration for a sequential task.

### 6.1 A. Task Decomposition and Rearrangement
![RHER_task](https://user-images.githubusercontent.com/28528386/218956945-77d1eff5-c153-40d7-a536-a2f2a6505c73.png)


Fig5. Sequential task decomposition and rearrangement.

### 6.2 B. Multi-goal & Multi-task RL Model.
![RHER_goal_encoding](https://user-images.githubusercontent.com/28528386/218956993-c763ab25-da0e-4a74-95ad-5927da6d553a.png)



Fig6. Multi-goal & Multi-task RL Model.


### 6.3 C. Maximize the Use of All Data by HER.
1. In the RHER framework, updating a policy can not only use its own explored data but also relabel the data collected by other policies by HER. 

2. Coincidentally, for continual RL, the agent also needs to generate non-negative samples by HER.

### 6.4 D. Self-Guided Exploration Strategy (SGES)

**Like students for scientific research, who are guided by advisers and other researchers until they need to explore a new field.**

![RHER-SGES](https://user-images.githubusercontent.com/28528386/218957053-cb0c035a-aab1-4ffe-a0b2-f7723cae82e9.png)


Fig7. Illustration of Self-Guided Exploration Strategy (SGES) in a toy push task. The black solid curve represents actual trajectory with SGES.

### 6.5 E. Relay Policy Learning.
![RHER_relay](https://user-images.githubusercontent.com/28528386/218957081-12a0961d-4d50-4c8f-9776-d08c72db6627.png)


Fig8. A diagram of relay policy learning for a task with 3 stages. By using HER and SGES, RHER can solve the whole sequential task stage by stage with sample efficient. 

## 7. Other interesting motivation:
1. Donâ€™t overambitious, agent need pay more attention to the goal which can be changed by itself.
2. One step at a time, gradually reach the distant goal.
3. Standing on the shoulders of giants, we can avoid many detours, just like scientific research.


## 8. Some interesting experiments that don't have space to show in the article:

1. Why learn a reach policy alone, instead of directly designing a simpler P-controller?

a) I really did do a comparison experiment~ In the manipulation tasks without obstacle, the effect of P-controller is not much different from that of RHER, and some are even faster because it can also reach the object quickly. 

But P-controller is much worse than RHER in tasks with obstacle, because RHER has the ability to adapt to the environment.

b) As for the tasks of multiple blocks, especially DPush, it is difficult to design a base controller that can push object1 to the specified position and reach the vicinity of object2, but RHER can deal with it.

## 9. Training Videos:
### 9.1 Training process for stack.

<details open="" class="details-reset border rounded-2">
  <summary class="px-3 py-2 border-bottom">
    <svg aria-hidden="true" viewBox="0 0 16 16" version="1.1" data-view-component="true" height="16" width="16" class="octicon octicon-device-camera-video">
    <path fill-rule="evenodd" d="..."></path>
</svg>
    <span aria-label="Video description Stack-RHER.mp4" class="m-1">RHER.mp4</span>
    <span class="dropdown-caret"></span>
  </summary>

  <video src="https://user-images.githubusercontent.com/28528386/192075197-11b1b6b1-3991-45da-ab75-4bed0cf10b54.mp4" data-canonical-src="https://user-images.githubusercontent.com/28528386/192075197-11b1b6b1-3991-45da-ab75-4bed0cf10b54.mp4" controls="controls" muted="muted" class="d-block rounded-bottom-2 width-fit" style="max-height:640px;">

  </video>
</details>


### 9.2 Training process for DrawerBox.

<details open="" class="details-reset border rounded-2">
  <summary class="px-3 py-2 border-bottom">
    <svg aria-hidden="true" viewBox="0 0 16 16" version="1.1" data-view-component="true" height="16" width="16" class="octicon octicon-device-camera-video">
    <path fill-rule="evenodd" d="..."></path>
</svg>
    <span aria-label="Video description Stack-RHER.mp4" class="m-1">RHER.mp4</span>
    <span class="dropdown-caret"></span>
  </summary>

  <video src="https://user-images.githubusercontent.com/28528386/193175188-b09d57cc-44c5-4609-9356-91bcbf2ba503.mp4" data-canonical-src="https://user-images.githubusercontent.com/28528386/193175188-b09d57cc-44c5-4609-9356-91bcbf2ba503.mp4" controls="controls" muted="muted" class="d-block rounded-bottom-2 width-fit" style="max-height:640px;">

  </video>
</details>

### 9.3 Training process for Real World Task.
<details open="" class="details-reset border rounded-2">
  <summary class="px-3 py-2 border-bottom">
    <svg aria-hidden="true" viewBox="0 0 16 16" version="1.1" data-view-component="true" height="16" width="16" class="octicon octicon-device-camera-video">
    <path fill-rule="evenodd" d="..."></path>
</svg>
    <span aria-label="Video description RHER.mp4" class="m-1">RHER.mp4</span>
    <span class="dropdown-caret"></span>
  </summary>

  <video src="https://user-images.githubusercontent.com/28528386/180405215-7410531f-01f3-41cf-bdae-808b896fb778.mp4" data-canonical-src="https://user-images.githubusercontent.com/28528386/180405215-7410531f-01f3-41cf-bdae-808b896fb778.mp4" controls="controls" muted="muted" class="d-block rounded-bottom-2 width-fit" style="max-height:640px;">

  </video>
</details>

### 9.4 Testing process of TPush and TStack with Success Rate about 80%.

<details open="" class="details-reset border rounded-2">
  <summary class="px-3 py-2 border-bottom">
    <svg aria-hidden="true" viewBox="0 0 16 16" version="1.1" data-view-component="true" height="16" width="16" class="octicon octicon-device-camera-video">
    <path fill-rule="evenodd" d="..."></path>
</svg>
    <span aria-label="Video description TStack-RHER.mp4" class="m-1">RHER.mp4</span>
    <span class="dropdown-caret"></span>
  </summary>

  <video src="https://user-images.githubusercontent.com/28528386/200292389-ddd96b5b-e57a-42b9-bc16-ff893c6b3b8c.mp4" data-canonical-src="https://user-images.githubusercontent.com/28528386/200292389-ddd96b5b-e57a-42b9-bc16-ff893c6b3b8c.mp4" controls="controls" muted="muted" class="d-block rounded-bottom-2 width-fit" style="max-height:640px;">

  </video>
</details>

## 11. Reproduce:

### Baselines
Our baselines is based on [OpenAI baselines](https://github.com/openai/baselines), and gym is based on [OpenAI gym](https://github.com/openai/gym/tree/0.18.0)

OpenAI Baselines is a set of high-quality implementations of reinforcement learning algorithms.

These algorithms will make it easier for the research community to replicate, refine, and identify new ideas, and will create good baselines to build research on top of. Our DQN implementation and its variants are roughly on par with the scores in published papers. We expect they will be used as a base around which new ideas can be added, and as a tool for comparing a new approach against existing ones. 

## Prerequisites 
Baselines requires python3 (>=3.5) with the development headers. You'll also need system packages CMake, OpenMPI and zlib. Those can be installed as follows
### Ubuntu 
    
```bash
sudo apt-get update && sudo apt-get install cmake libopenmpi-dev python3-dev zlib1g-dev
```

## Virtual environment
```bash
conda create -n rher python=3.6
```

## Tensorflow versions
The master branch supports Tensorflow 1.14.

## Installation
- Clone the repo and cd into it:
    ```bash
    git clone https://github.com/kaixindelele/RHER.git
    cd RHER
    ```
- If you don't have TensorFlow installed already, install your favourite flavor of TensorFlow. In most cases, you may use
    ```bash 
    conda install tensorflow-gpu==1.14 # if you have a CUDA-compatible gpu and proper drivers
    ```
    
    and
    
    ```bash 
    pip install -r requirement.txt
    ```
    

### MuJoCo (200)
Some of the baselines examples use [MuJoCo](http://www.mujoco.org) (multi-joint dynamics in contact) physics simulator, which is proprietary and requires binaries and a license (license can be obtained from [mujoco-free-license](https://github.com/kaixindelele/RHER/blob/main/gym/mjkey.txt)

### MuJoCo-py (2.0.2.1)
Instructions on setting up MuJoCo can be found [mujoco-py(2.0.2.1)](https://github.com/openai/mujoco-py/tree/v2.0.2.1)

## Training models
run in terminal
```bash
bash run_rher_push.sh
```

or
run in pycharm
```bash
python -m baselines.run_rher_np1.py
```


# Starchart

[![Star History Chart](https://api.star-history.com/svg?repos=kaixindelele/RHER&type=Date)](https://star-history.com/#kaixindelele/RHER&Date)
